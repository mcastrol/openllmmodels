{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test questions using RAG\n",
    "- Vector db: qdrant\n",
    "--OpenIA LLM model / Embedding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import textwrap\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "QDRANT_URL= os.getenv('QDRANT_URL')\n",
    "QDRANT_API_KEY=\"\"\n",
    "collection_name = os.getenv('QDRANT_TABLE_NAME')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(retriever_model, qdrant_client, question, collection_name):\n",
    "    # encoded_query = get_embeddings(question)  # generate embeddings for the question\n",
    "    encoded_query = retriever_model.embed_query(question)\n",
    "    prompt_dont_know = \\\n",
    "        \"\"\"\n",
    "            You are a helpful AI language model. \n",
    "            Politely state that you do not know the answer and you are doing best to learn new documents to answer future questions.\n",
    "        \"\"\"\n",
    "    explanation_docs = []\n",
    "    if encoded_query is None:\n",
    "        return prompt_dont_know, explanation_docs\n",
    "    result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=encoded_query,\n",
    "        limit=7, score_threshold=0.7\n",
    "    )\n",
    "    # print(f\"Result from qdrant - Items found {len(result)}\")\n",
    "    # for text in result:\n",
    "    #     print(text.payload['page_content'])\n",
    "    if len(result) == 0:\n",
    "        return prompt_dont_know, explanation_docs\n",
    "    prompt_template = \\\n",
    "        \"\"\"\n",
    "            You are an specialist in answering questions about databricks\n",
    "            Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "            Answer in the same language the question was asked.\n",
    "            Where possible, be specific, provide examples, and explain your thinking.\n",
    "            \n",
    "            # context from our documents\n",
    "            {context}\n",
    "            \n",
    "            # question to answer.\n",
    "            {question}\n",
    "            \n",
    "        \"\"\"\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    prompt = PROMPT.format(question=question, context=\"\\n\\n\".join([text.payload['page_content'] for text in result]))\n",
    "    explanation_docs = list(set([text.payload[\"metadata\"]['filename'].split(\"/\")[-1] for text in result]))\n",
    "    return prompt,explanation_docs\n",
    "\n",
    "\n",
    "def serve_data_streaming(llmclient, model, prompt):\n",
    "    stream = llmclient.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            yield chunk.choices[0].delta.content\n",
    "\n",
    "def serve_data(llmclient, model, prompt):\n",
    "    response = llmclient.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "def print_wrapped(text, width=50):\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "    print(wrapped_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcastrol/github/openllmmodels/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/mcastrol/github/openllmmodels/.venv/lib/python3.11/site-packages/qdrant_client/qdrant_remote.py:130: UserWarning: Api key is used with an insecure connection.\n",
      "  warnings.warn(\"Api key is used with an insecure connection.\")\n"
     ]
    }
   ],
   "source": [
    "#set openai api key\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#set OpenAI embedding model \n",
    "embeddings = OpenAIEmbeddings(\n",
    "            openai_api_key=api_key\n",
    "        )\n",
    "\n",
    "#vector db client\n",
    "qdrant_client = QdrantClient(url=QDRANT_URL,\n",
    "                             port=None,\n",
    "                             api_key=QDRANT_API_KEY)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Databricks-Big-Book-Of-GenAI-FINAL.pdf']\n",
      "\n",
      "            You are an specialist in answering questions about databricks\n",
      "            Don't try to make up an answer, if you don't know just say that you don't know.\n",
      "            Answer in the same language the question was asked.\n",
      "            Where possible, be specific, provide examples, and explain your thinking.\n",
      "            \n",
      "            # context from our documents\n",
      "            In this eBook, you’ll learn:\n",
      "\n",
      "the incredible insights you’ll gain!\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "the following:\n",
      "            \n",
      "            # question to answer.\n",
      "            In this eBook, you’ll learn:?\n",
      "            \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "#prepare a question an call get prompt to embed question and extract top question from vector db and the prompt to call llm\n",
    "question=\"In this eBook, you’ll learn:?\"\n",
    "prompt,explanation_docs=get_prompt(embeddings, qdrant_client, question, collection_name)\n",
    "print(explanation_docs)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call to LLM with the dynamic prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "model=\"gpt-3.5-turbo-0125\"\n",
    "\n",
    "response=serve_data(client, model, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will learn about the incredible insights\n",
      "you'll gain from the content of this eBook. It\n",
      "will provide valuable information and knowledge\n",
      "that will help you understand specific topics in-\n",
      "depth and gain new perspectives. For example, if\n",
      "the eBook is about data analysis using Databricks,\n",
      "you might learn about advanced data manipulation\n",
      "techniques, machine learning algorithms, and best\n",
      "practices for optimizing data processing\n",
      "workflows.\n"
     ]
    }
   ],
   "source": [
    "print_wrapped(response, width=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
