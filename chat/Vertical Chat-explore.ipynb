{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/main/1-Introduction%20to%20LLMs%20with%20OpenAI/Vertical%20Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zwzWskLgXgj5",
      "metadata": {
        "id": "zwzWskLgXgj5"
      },
      "source": [
        "<div align=\"center\">\n",
        "<h1><a href=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course\">Learn by Doing LLM Projects</a></h1>\n",
        "    <h3>Understand And Apply Large Language Models</h3>\n",
        "    <h2>Create your first ChatBot with OpenAI</h2>\n",
        "    <h3>Using GPT 3.5, Python and Panel</h3>\n",
        "    by <b>Pere Martra</b>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "    &nbsp;\n",
        "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/pere-martra/\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c23b4dd1",
      "metadata": {
        "id": "c23b4dd1"
      },
      "source": [
        "# Vertical Chat\n",
        "A sample how to build a chat for small businees using:\n",
        "\n",
        "* GPT 35\n",
        "* Panel\n",
        "* OpenAI\n",
        "\n",
        "\n",
        "This is just a simple sample to start to understand how the OpenAI API works, and how to create Prompts. It Is really far from beign a complete solution.\n",
        "We are going to introduce some interesting points:\n",
        "\n",
        "* The roles in a conversation.\n",
        "* How is the conversationsâ€™ memory preserved?\n",
        "\n",
        "Deeper explanations in the article: [Create Your First Chatbot Using GPT 3.5, OpenAI, Python and Panel.](https://medium.com/towards-artificial-intelligence/create-your-first-chatbot-using-gpt-3-5-openai-python-and-panel-7ec180b9d7f2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a1d00720",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1d00720",
        "outputId": "507ec869-bd20-4837-e731-4188221494dd"
      },
      "outputs": [],
      "source": [
        "#First install the necessary libraries\n",
        "# !pip install openai==1.1.1\n",
        "# !pip install panel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a03f026a",
      "metadata": {
        "id": "a03f026a"
      },
      "outputs": [],
      "source": [
        "#if you need an API Key from OpenAI\n",
        "#https://platform.openai.com/account/api-keys\n",
        "\n",
        "import openai\n",
        "import panel as pn\n",
        "\n",
        "# #from mykeys import openai_api_key\n",
        "# openai.api_key=\"your-api-key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "77eac86d",
      "metadata": {
        "id": "77eac86d"
      },
      "outputs": [],
      "source": [
        "def continue_conversation(messages, temperature=0):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    #print(str(response.choices[0].message[\"content\"]))\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "922f8d24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "922f8d24",
        "outputId": "93183987-3b29-4430-d8cc-6d6453a98574"
      },
      "outputs": [],
      "source": [
        "#Creating the prompt\n",
        "#read and understand it.\n",
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "Act as an OrderBot, you work collecting orders in a delivery only fast food restaurant called\n",
        "My Dear Frankfurt. \\\n",
        "First welcome the customer, in a very friendly way, then collects the order. \\\n",
        "You wait to collect the entire order, beverages included \\\n",
        "then summarize it and check for a final \\\n",
        "time if everything is ok or the customer wants to add anything else. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, very friendly style. \\\n",
        "The menu includes \\\n",
        "burger  12.95, 10.00, 7.00 \\\n",
        "frankfurt   10.95, 9.25, 6.50 \\\n",
        "sandwich   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "martra sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "romesco sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "vichy catalan 5.00 \\\n",
        "\"\"\"} ]\n",
        "\n",
        "#Creating the panel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "26ddfbee",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_user=\"HI how are you\"\n",
        "context.append({'role':'user', 'content':f\"{input_user}\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "54211541",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = continue_conversation(messages=context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "79a1b11b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hello! I'm great, thank you for asking. Welcome to My Dear Frankfurt! How can I assist you today?\""
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "685cf925",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f64d670171b74129a7c06a5bfb38f7ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cpu\" # the device to load the model onto\n",
        "model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f7d2b002",
      "metadata": {},
      "outputs": [],
      "source": [
        "def continue_conversation_mistral(tokenizer, messages, temperature=0):\n",
        "    device='cpu' \n",
        "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = encodeds.to(device)\n",
        "    model.to(device)\n",
        "    generated_ids = model.generate(model_inputs,pad_token_id = tokenizer.eos_token_id)\n",
        "    decoded = tokenizer.batch_decode(generated_ids)\n",
        "    print(decoded[0])\n",
        "    return(decoded[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8a159f85",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_user=\"HI how are you\"\n",
        "context.append({'role':'assistant', 'content':\"I'm your asssistant for help tyou to order your meal today\"})\n",
        "context.append({'role':'user', 'content':f\"{input_user}\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "416283fe",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': '\\nAct as an OrderBot, you work collecting orders in a delivery only fast food restaurant called\\nMy Dear Frankfurt. First welcome the customer, in a very friendly way, then collects the order. You wait to collect the entire order, beverages included then summarize it and check for a final time if everything is ok or the customer wants to add anything else. Finally you collect the payment.Make sure to clarify all options, extras and sizes to uniquely identify the item from the menu.You respond in a short, very friendly style. The menu includes burger  12.95, 10.00, 7.00 frankfurt   10.95, 9.25, 6.50 sandwich   11.95, 9.75, 6.75 fries 4.50, 3.50 salad 7.25 Toppings: extra cheese 2.00, mushrooms 1.50 martra sausage 3.00 canadian bacon 3.50 romesco sauce 1.50 peppers 1.00 Drinks: coke 3.00, 2.00, 1.00 sprite 3.00, 2.00, 1.00 vichy catalan 5.00 '},\n",
              " {'role': 'assistant',\n",
              "  'content': \"I'm your asssistant for help tyou to order your meal today\"},\n",
              " {'role': 'user', 'content': 'HI how are you'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"I'm your asssistant for help tyou to order your meal today\"},\n",
              " {'role': 'user', 'content': 'HI how are you'}]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "1a8668ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "context = [\n",
        "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "196ddd2f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/home/mcastrol/github/openllmmodels/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Input length of input_ids is 70, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontinue_conversation_mistral\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[26], line 6\u001b[0m, in \u001b[0;36mcontinue_conversation_mistral\u001b[0;34m(tokenizer, messages, temperature)\u001b[0m\n\u001b[1;32m      4\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m encodeds\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoded[\u001b[38;5;241m0\u001b[39m])\n",
            "File \u001b[0;32m~/github/openllmmodels/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/github/openllmmodels/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1449\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1444\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `generation_config` defines a `cache_implementation` that is not compatible with this model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1445\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Make sure it has a `_setup_cache` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1446\u001b[0m             )\n\u001b[1;32m   1447\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_cache(cache_cls, max_batch_size\u001b[38;5;241m=\u001b[39mbatch_size, max_cache_len\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m-> 1449\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mget_generation_mode(assistant_model)\n",
            "File \u001b[0;32m~/github/openllmmodels/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1140\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1139\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1142\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1150\u001b[0m )\n",
            "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 70, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
          ]
        }
      ],
      "source": [
        "response = continue_conversation_mistral(tokenizer,messages=context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bf47403",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f00ba09",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
